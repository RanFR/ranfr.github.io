---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

<span class='anchor' id='about-me'/>

# 👨‍🎓 个人简介

我是[北京理工大学](https://www.bit.edu.cn)自动化专业的二年级硕士研究生，作为**自主智能无人系统国家重点实验室**的一员，专注于无人机/无人车路径规划及其支撑技术，如SLAM、运动控制及多机器人协作。我于2023年获得*北京理工大学(BIT)*自动化专业的工学学士学位。

我的研究兴趣包括**无人机/无人车路径规划与运动控制**等，并在这些方面具有丰富的动手能力与项目经验。我在国际会议上发表过多篇论文，包括IEEE ICCA以及IEEE RCAR。

<span class="anchor" id="education">

# 📖 教育经历

- *2019年9月 - 2023年6月*  本科  北京理工大学  **徐特立学院** 自动化 ('**双一流学科**'，'**A+**')

  核心课程

  学分绩：90.4/100

- *2023年9月 - 至今* 硕士(**保研**) 北京理工大学  自动化学院  控制工程  ('**双一流学科**'，'**A+**')

  学分绩：86.13/100

<span class="anchor" id="projects">

# 📁 项目经历

  - *2021年10月 - 2025年3月*, 空地异构平台协同的室内导航与侦测（**国家重点研发项目**）

    项目描述：
    面向操作人员难以进入的复杂场景（如震后受损建筑），使用多个无人机平台，根据环境信息进行完全自主决策，探索目标场景并构建地图。

    个人工作：设计项目子模块状态机实现与无人机探索任务中的航迹规划。
      1. 负责设计规划子模块的状态机，便于进行规划与探索模式的切换；
      2. 在完全未知环境中，参考无人机自身搭载的感知设备，设计算法获取起点到终点的有效路径，用于后续优化；
      3. 构建局部ESDF地图，建立避障轨迹的非线性优化模型，得到一条平滑、无障碍、满足动力学约束的路径。

    项目成果：室内建图最大误差35cm。

<span class="anchor" id="papers"/>

# 📝 论文发表

<div class="paper-box">
  <div class="paper-box-image">
    <div class="badge">ICCA 2025</div>
    <img src="images/icca-paper.png" alt="sym" width="100%">
  </div>
  <div class="paper-box-text">
    <a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">
      <strong>Autonomous UAV Path Planning in Dynamic Environments: A Hybrid Framework of Trajectory Prediction and Priority-Aware DWA</strong>
    </a>
    <p><strong>Fengrui Ran</strong>, Chengpu Yu, Erpei Xu, Yunji Feng</p>
    <a href="https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC">
      <strong>工作创新</strong>
    </a>
    <ul>
      <li>1</li>
      <li>2</li>
      <li>3</li>
    </ul>
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-image">
    <div class="badge">RCAR 2025</div>
    <img src="images/rcar-paper.png" alt="sym" width="100%">
  </div>
  <div class="paper-box-text">
    <a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">
      <strong>ExploreGS: a vision-based low overhead framework for 3D scene reconstruction</strong>
    </a>
    <p>Yunji Feng, Chengpu Yu, <strong>Fengrui Ran</strong>, Zhi Yang, Yinni Liu</p>
    <a href="https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC">
      <strong>参与工作</strong>
    </a>
    <ul>
      <li>1</li>
      <li>2</li>
      <li>3</li>
    </ul>
  </div>
</div>

<span class="anchor" id="awards"/>

# 🎖 荣誉奖项
- *2024* 中国机器人及人工智能大赛**一等奖**（**国家级**）
- *2024* 中国研究生数学建模竞赛**三等奖**（**国家级**）
- *2023* 全国大学生课外学术科技作品竞赛**一等奖**（院校级）
- *2023* “互联网+”大学生创新创业大赛**铜奖**（院校级）

<!-- # 📖 Educations
- *2019.06 - 2022.04 (now)*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.
- *2015.09 - 2019.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. -->
<!--
# 💬 Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/) -->

<!-- # 💻 Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China. -->
